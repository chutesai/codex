diff --git a/codex-rs/core/src/client.rs b/codex-rs/core/src/client.rs
index ec0d1e54f..19a8f9181 100644
--- a/codex-rs/core/src/client.rs
+++ b/codex-rs/core/src/client.rs
@@ -64,6 +64,7 @@ use codex_otel::OtelManager;
 use codex_protocol::ThreadId;
 use codex_protocol::config_types::ReasoningSummary as ReasoningSummaryConfig;
 use codex_protocol::config_types::Verbosity as VerbosityConfig;
+use codex_protocol::models::ContentItem;
 use codex_protocol::models::ResponseItem;
 use codex_protocol::openai_models::ModelInfo;
 use codex_protocol::openai_models::ReasoningEffort as ReasoningEffortConfig;
@@ -448,6 +449,119 @@ impl ModelClient {
 }
 
 impl ModelClientSession {
+    fn provider_is_chutes_responses_proxy(provider: &codex_api::Provider) -> bool {
+        let base_url = provider.base_url.trim_end_matches('/').to_ascii_lowercase();
+        base_url.starts_with("https://responses.chutes.ai")
+            || base_url.starts_with("http://responses.chutes.ai")
+    }
+
+    fn downgrade_developer_messages(provider: &codex_api::Provider, input: &mut [ResponseItem]) {
+        // The Chutes Responses proxy rejects the newer `developer` role used by OpenAI Responses
+        // API, but still accepts `system`.
+        if !Self::provider_is_chutes_responses_proxy(provider) {
+            return;
+        }
+
+        for item in input {
+            if let ResponseItem::Message { role, .. } = item
+                && role == "developer"
+            {
+                *role = "system".to_string();
+            }
+        }
+    }
+
+    fn fold_system_messages_into_instructions(
+        provider: &codex_api::Provider,
+        instructions: &mut String,
+        input: &mut Vec<ResponseItem>,
+    ) {
+        if !Self::provider_is_chutes_responses_proxy(provider) {
+            return;
+        }
+
+        let mut system_messages = Vec::new();
+        input.retain(|item| {
+            let ResponseItem::Message { role, content, .. } = item else {
+                return true;
+            };
+            if role != "system" {
+                return true;
+            }
+
+            let text = content
+                .iter()
+                .filter_map(|item| match item {
+                    ContentItem::InputText { text } | ContentItem::OutputText { text } => {
+                        Some(text.as_str())
+                    }
+                    ContentItem::InputImage { .. } => None,
+                })
+                .collect::<Vec<_>>()
+                .join("\n");
+            if !text.is_empty() {
+                system_messages.push(text);
+            }
+
+            false
+        });
+
+        if system_messages.is_empty() {
+            return;
+        }
+
+        if !instructions.ends_with('\n') {
+            instructions.push('\n');
+        }
+        for message in system_messages {
+            instructions.push('\n');
+            instructions.push_str(&message);
+            if !instructions.ends_with('\n') {
+                instructions.push('\n');
+            }
+        }
+    }
+
+    fn filter_tools_for_proxy(provider: &codex_api::Provider, tools: &mut Vec<serde_json::Value>) {
+        if !Self::provider_is_chutes_responses_proxy(provider) {
+            return;
+        }
+
+        // Some third-party endpoints only support `function` tools and will either reject or
+        // silently disable structured tool calling when non-function tool types (e.g. `web_search`)
+        // are present.
+        tools.retain(|tool| {
+            tool.get("type")
+                .and_then(serde_json::Value::as_str)
+                .is_some_and(|ty| ty == "function")
+        });
+    }
+
+    fn append_proxy_tool_call_instructions(
+        provider: &codex_api::Provider,
+        instructions: &mut String,
+    ) {
+        if !Self::provider_is_chutes_responses_proxy(provider) {
+            return;
+        }
+
+        // Many OpenAI-compatible backends (and responses proxies) surface tool calls as plain text
+        // instead of structured `function_call` output items. Ensure a stable tag format that
+        // clients can parse.
+        instructions.push_str(
+            r#"
+
+Tool calls (important):
+- When you need to call a tool, output ONLY one or more blocks in this exact format:
+  <function=TOOL_NAME>
+  json
+  {...}
+- Do not use the format <function>TOOL_NAME</function>.
+- Do not add any other text outside the <function=...> blocks.
+"#,
+        );
+    }
+
     fn activate_http_fallback(&self, websocket_enabled: bool) -> bool {
         websocket_enabled
             && !self
@@ -465,9 +579,13 @@ impl ModelClientSession {
         effort: Option<ReasoningEffortConfig>,
         summary: ReasoningSummaryConfig,
     ) -> Result<ResponsesApiRequest> {
-        let instructions = &prompt.base_instructions.text;
-        let input = prompt.get_formatted_input();
-        let tools = create_tools_json_for_responses_api(&prompt.tools)?;
+        let mut instructions = prompt.base_instructions.text.clone();
+        let mut input = prompt.get_formatted_input();
+        Self::downgrade_developer_messages(provider, &mut input);
+        Self::fold_system_messages_into_instructions(provider, &mut instructions, &mut input);
+        Self::append_proxy_tool_call_instructions(provider, &mut instructions);
+        let mut tools = create_tools_json_for_responses_api(&prompt.tools)?;
+        Self::filter_tools_for_proxy(provider, &mut tools);
         let default_reasoning_effort = model_info.default_reasoning_level;
         let reasoning = if model_info.supports_reasoning_summaries {
             Some(Reasoning {
@@ -504,7 +622,7 @@ impl ModelClientSession {
         let prompt_cache_key = Some(self.client.state.conversation_id.to_string());
         let request = ResponsesApiRequest {
             model: model_info.slug.clone(),
-            instructions: instructions.clone(),
+            instructions,
             input,
             tools,
             tool_choice: "auto".to_string(),
@@ -1162,13 +1280,19 @@ impl WebsocketTelemetry for ApiTelemetry {
 #[cfg(test)]
 mod tests {
     use super::ModelClient;
+    use super::ModelClientSession;
+    use codex_api::provider::RetryConfig;
     use codex_otel::OtelManager;
     use codex_protocol::ThreadId;
+    use codex_protocol::models::ContentItem;
+    use codex_protocol::models::ResponseItem;
     use codex_protocol::openai_models::ModelInfo;
     use codex_protocol::protocol::SessionSource;
     use codex_protocol::protocol::SubAgentSource;
+    use http::HeaderMap;
     use pretty_assertions::assert_eq;
     use serde_json::json;
+    use std::time::Duration;
 
     fn test_model_client(session_source: SessionSource) -> ModelClient {
         let provider = crate::model_provider_info::create_oss_provider_with_base_url(
@@ -1257,4 +1381,246 @@ mod tests {
             .expect("empty summarize request should succeed");
         assert_eq!(output.len(), 0);
     }
+
+    #[test]
+    fn downgrades_developer_messages_for_non_openai_base_urls() {
+        let provider = codex_api::Provider {
+            name: "proxy".to_string(),
+            base_url: "https://responses.chutes.ai/v1".to_string(),
+            query_params: None,
+            headers: HeaderMap::new(),
+            retry: RetryConfig {
+                max_attempts: 1,
+                base_delay: Duration::from_millis(1),
+                retry_429: false,
+                retry_5xx: false,
+                retry_transport: false,
+            },
+            stream_idle_timeout: Duration::from_secs(30),
+        };
+
+        let mut items = vec![
+            ResponseItem::Message {
+                id: None,
+                role: "developer".to_string(),
+                content: vec![ContentItem::InputText {
+                    text: "dev".to_string(),
+                }],
+                end_turn: None,
+                phase: None,
+            },
+            ResponseItem::Message {
+                id: None,
+                role: "user".to_string(),
+                content: vec![ContentItem::InputText {
+                    text: "user".to_string(),
+                }],
+                end_turn: None,
+                phase: None,
+            },
+        ];
+
+        ModelClientSession::downgrade_developer_messages(&provider, &mut items);
+
+        let ResponseItem::Message { role, .. } = &items[0] else {
+            panic!("expected message item");
+        };
+        assert_eq!(role, "system");
+
+        let ResponseItem::Message { role, .. } = &items[1] else {
+            panic!("expected message item");
+        };
+        assert_eq!(role, "user");
+    }
+
+    #[test]
+    fn keeps_developer_messages_for_openai_base_urls() {
+        let provider = codex_api::Provider {
+            name: "openai".to_string(),
+            base_url: "https://api.openai.com/v1".to_string(),
+            query_params: None,
+            headers: HeaderMap::new(),
+            retry: RetryConfig {
+                max_attempts: 1,
+                base_delay: Duration::from_millis(1),
+                retry_429: false,
+                retry_5xx: false,
+                retry_transport: false,
+            },
+            stream_idle_timeout: Duration::from_secs(30),
+        };
+
+        let mut items = vec![ResponseItem::Message {
+            id: None,
+            role: "developer".to_string(),
+            content: vec![ContentItem::InputText {
+                text: "dev".to_string(),
+            }],
+            end_turn: None,
+            phase: None,
+        }];
+
+        ModelClientSession::downgrade_developer_messages(&provider, &mut items);
+
+        let ResponseItem::Message { role, .. } = &items[0] else {
+            panic!("expected message item");
+        };
+        assert_eq!(role, "developer");
+    }
+
+    #[test]
+    fn folds_system_messages_into_instructions_for_non_openai_base_urls() {
+        let provider = codex_api::Provider {
+            name: "proxy".to_string(),
+            base_url: "https://responses.chutes.ai/v1".to_string(),
+            query_params: None,
+            headers: HeaderMap::new(),
+            retry: RetryConfig {
+                max_attempts: 1,
+                base_delay: Duration::from_millis(1),
+                retry_429: false,
+                retry_5xx: false,
+                retry_transport: false,
+            },
+            stream_idle_timeout: Duration::from_secs(30),
+        };
+
+        let mut instructions = "base".to_string();
+        let mut input = vec![
+            ResponseItem::Message {
+                id: None,
+                role: "system".to_string(),
+                content: vec![ContentItem::InputText {
+                    text: "sys".to_string(),
+                }],
+                end_turn: None,
+                phase: None,
+            },
+            ResponseItem::Message {
+                id: None,
+                role: "user".to_string(),
+                content: vec![ContentItem::InputText {
+                    text: "user".to_string(),
+                }],
+                end_turn: None,
+                phase: None,
+            },
+        ];
+
+        ModelClientSession::fold_system_messages_into_instructions(
+            &provider,
+            &mut instructions,
+            &mut input,
+        );
+
+        assert!(instructions.contains("sys"));
+        assert_eq!(input.len(), 1);
+        let ResponseItem::Message { role, .. } = &input[0] else {
+            panic!("expected message item");
+        };
+        assert_eq!(role, "user");
+    }
+
+    #[test]
+    fn keeps_system_messages_in_input_for_openai_base_urls() {
+        let provider = codex_api::Provider {
+            name: "openai".to_string(),
+            base_url: "https://api.openai.com/v1".to_string(),
+            query_params: None,
+            headers: HeaderMap::new(),
+            retry: RetryConfig {
+                max_attempts: 1,
+                base_delay: Duration::from_millis(1),
+                retry_429: false,
+                retry_5xx: false,
+                retry_transport: false,
+            },
+            stream_idle_timeout: Duration::from_secs(30),
+        };
+
+        let mut instructions = "base".to_string();
+        let mut input = vec![ResponseItem::Message {
+            id: None,
+            role: "system".to_string(),
+            content: vec![ContentItem::InputText {
+                text: "sys".to_string(),
+            }],
+            end_turn: None,
+            phase: None,
+        }];
+
+        ModelClientSession::fold_system_messages_into_instructions(
+            &provider,
+            &mut instructions,
+            &mut input,
+        );
+
+        assert_eq!(instructions, "base");
+        assert_eq!(input.len(), 1);
+        let ResponseItem::Message { role, .. } = &input[0] else {
+            panic!("expected message item");
+        };
+        assert_eq!(role, "system");
+    }
+
+    #[test]
+    fn filters_non_function_tools_for_non_openai_base_urls() {
+        let provider = codex_api::Provider {
+            name: "proxy".to_string(),
+            base_url: "https://responses.chutes.ai/v1".to_string(),
+            query_params: None,
+            headers: HeaderMap::new(),
+            retry: RetryConfig {
+                max_attempts: 1,
+                base_delay: Duration::from_millis(1),
+                retry_429: false,
+                retry_5xx: false,
+                retry_transport: false,
+            },
+            stream_idle_timeout: Duration::from_secs(30),
+        };
+
+        let mut tools = vec![
+            json!({"type":"function","function":{"name":"shell"}}),
+            json!({"type":"web_search"}),
+            json!({"type":"file_search"}),
+            json!({}),
+        ];
+
+        ModelClientSession::filter_tools_for_proxy(&provider, &mut tools);
+
+        assert_eq!(
+            tools,
+            vec![json!({"type":"function","function":{"name":"shell"}})]
+        );
+    }
+
+    #[test]
+    fn keeps_non_function_tools_for_openai_base_urls() {
+        let provider = codex_api::Provider {
+            name: "openai".to_string(),
+            base_url: "https://api.openai.com/v1".to_string(),
+            query_params: None,
+            headers: HeaderMap::new(),
+            retry: RetryConfig {
+                max_attempts: 1,
+                base_delay: Duration::from_millis(1),
+                retry_429: false,
+                retry_5xx: false,
+                retry_transport: false,
+            },
+            stream_idle_timeout: Duration::from_secs(30),
+        };
+
+        let mut tools = vec![
+            json!({"type":"function","function":{"name":"shell"}}),
+            json!({"type":"web_search"}),
+            json!({"type":"file_search"}),
+            json!({}),
+        ];
+
+        ModelClientSession::filter_tools_for_proxy(&provider, &mut tools);
+
+        assert_eq!(tools.len(), 4);
+    }
 }
diff --git a/codex-rs/core/src/codex.rs b/codex-rs/core/src/codex.rs
index 87ae68f2d..4a46847a7 100644
--- a/codex-rs/core/src/codex.rs
+++ b/codex-rs/core/src/codex.rs
@@ -5387,7 +5387,7 @@ async fn try_run_sampling_request(
                 let output_result = handle_output_item_done(&mut ctx, item, previously_active_item)
                     .instrument(handle_responses)
                     .await?;
-                if let Some(tool_future) = output_result.tool_future {
+                for tool_future in output_result.tool_futures {
                     in_flight.push_back(tool_future);
                 }
                 if let Some(agent_message) = output_result.last_agent_message {
diff --git a/codex-rs/core/src/models_manager/model_info.rs b/codex-rs/core/src/models_manager/model_info.rs
index 6a29cd96f..672aacd9e 100644
--- a/codex-rs/core/src/models_manager/model_info.rs
+++ b/codex-rs/core/src/models_manager/model_info.rs
@@ -1,3 +1,4 @@
+use codex_protocol::openai_models::ApplyPatchToolType;
 use codex_protocol::openai_models::ConfigShellToolType;
 use codex_protocol::openai_models::ModelInfo;
 use codex_protocol::openai_models::ModelInstructionsVariables;
@@ -18,6 +19,7 @@ const LOCAL_FRIENDLY_TEMPLATE: &str =
     "You optimize for team morale and being a supportive teammate as much as code quality.";
 const LOCAL_PRAGMATIC_TEMPLATE: &str = "You are a deeply pragmatic, effective software engineer.";
 const PERSONALITY_PLACEHOLDER: &str = "{{ personality }}";
+const FALLBACK_EXPERIMENTAL_TOOLS: [&str; 3] = ["grep_files", "list_dir", "read_file"];
 
 pub(crate) fn with_config_overrides(mut model: ModelInfo, config: &Config) -> ModelInfo {
     if let Some(supports_reasoning_summaries) = config.model_supports_reasoning_summaries {
@@ -72,18 +74,26 @@ pub(crate) fn model_info_from_slug(slug: &str) -> ModelInfo {
         supports_reasoning_summaries: false,
         support_verbosity: false,
         default_verbosity: None,
-        apply_patch_tool_type: None,
+        // Unknown models are frequently third-party and still need core coding tools.
+        apply_patch_tool_type: Some(ApplyPatchToolType::Function),
         truncation_policy: TruncationPolicyConfig::bytes(10_000),
-        supports_parallel_tool_calls: false,
+        supports_parallel_tool_calls: true,
         context_window: Some(272_000),
         auto_compact_token_limit: None,
         effective_context_window_percent: 95,
-        experimental_supported_tools: Vec::new(),
+        experimental_supported_tools: fallback_experimental_supported_tools(),
         input_modalities: default_input_modalities(),
         prefer_websockets: false,
     }
 }
 
+fn fallback_experimental_supported_tools() -> Vec<String> {
+    FALLBACK_EXPERIMENTAL_TOOLS
+        .into_iter()
+        .map(str::to_string)
+        .collect()
+}
+
 fn local_personality_messages_for_slug(slug: &str) -> Option<ModelMessages> {
     match slug {
         "gpt-5.2-codex" | "exp-codex-personality" => Some(ModelMessages {
@@ -99,3 +109,45 @@ fn local_personality_messages_for_slug(slug: &str) -> Option<ModelMessages> {
         _ => None,
     }
 }
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use pretty_assertions::assert_eq;
+
+    #[test]
+    fn unknown_model_fallback_enables_proxy_compatible_tools() {
+        let slug = "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8";
+        let model = model_info_from_slug(slug);
+
+        assert_eq!(
+            model,
+            ModelInfo {
+                slug: slug.to_string(),
+                display_name: slug.to_string(),
+                description: None,
+                default_reasoning_level: None,
+                supported_reasoning_levels: Vec::new(),
+                shell_type: ConfigShellToolType::Default,
+                visibility: ModelVisibility::None,
+                supported_in_api: true,
+                priority: 99,
+                upgrade: None,
+                base_instructions: BASE_INSTRUCTIONS.to_string(),
+                model_messages: None,
+                supports_reasoning_summaries: false,
+                support_verbosity: false,
+                default_verbosity: None,
+                apply_patch_tool_type: Some(ApplyPatchToolType::Function),
+                truncation_policy: TruncationPolicyConfig::bytes(10_000),
+                supports_parallel_tool_calls: true,
+                context_window: Some(272_000),
+                auto_compact_token_limit: None,
+                effective_context_window_percent: 95,
+                experimental_supported_tools: fallback_experimental_supported_tools(),
+                input_modalities: default_input_modalities(),
+                prefer_websockets: false,
+            }
+        );
+    }
+}
diff --git a/codex-rs/core/src/stream_events_utils.rs b/codex-rs/core/src/stream_events_utils.rs
index 394f4b932..dd9d6f05d 100644
--- a/codex-rs/core/src/stream_events_utils.rs
+++ b/codex-rs/core/src/stream_events_utils.rs
@@ -13,14 +13,17 @@ use crate::function_tool::FunctionCallError;
 use crate::parse_turn_item;
 use crate::proposed_plan_parser::strip_proposed_plan_blocks;
 use crate::tools::parallel::ToolCallRuntime;
+use crate::tools::router::ToolCall;
 use crate::tools::router::ToolRouter;
 use codex_protocol::models::FunctionCallOutputBody;
 use codex_protocol::models::FunctionCallOutputPayload;
 use codex_protocol::models::ResponseInputItem;
 use codex_protocol::models::ResponseItem;
 use futures::Future;
+use serde_json::Value;
 use tracing::debug;
 use tracing::instrument;
+use uuid::Uuid;
 
 /// Handle a completed output item from the model stream, recording it and
 /// queuing any tool execution futures. This records items immediately so
@@ -32,7 +35,7 @@ pub(crate) type InFlightFuture<'f> =
 pub(crate) struct OutputItemResult {
     pub last_agent_message: Option<String>,
     pub needs_follow_up: bool,
-    pub tool_future: Option<InFlightFuture<'static>>,
+    pub tool_futures: Vec<InFlightFuture<'static>>,
 }
 
 pub(crate) struct HandleOutputCtx {
@@ -42,6 +45,187 @@ pub(crate) struct HandleOutputCtx {
     pub cancellation_token: CancellationToken,
 }
 
+fn base_url_is_chutes_responses_proxy(base_url: &str) -> bool {
+    let base_url = base_url.trim_end_matches('/').to_ascii_lowercase();
+    base_url.starts_with("https://responses.chutes.ai")
+        || base_url.starts_with("http://responses.chutes.ai")
+}
+
+fn should_parse_proxy_tool_calls(provider: &crate::model_provider_info::ModelProviderInfo) -> bool {
+    provider
+        .base_url
+        .as_deref()
+        .is_some_and(base_url_is_chutes_responses_proxy)
+}
+
+fn parse_proxy_function_calls(item: &ResponseItem) -> Option<Vec<(String, String)>> {
+    let ResponseItem::Message { role, content, .. } = item else {
+        return None;
+    };
+    if role != "assistant" {
+        return None;
+    }
+
+    let combined = content
+        .iter()
+        .filter_map(|entry| match entry {
+            codex_protocol::models::ContentItem::OutputText { text } => Some(text.as_str()),
+            _ => None,
+        })
+        .collect::<String>();
+    if combined.is_empty() {
+        return None;
+    }
+
+    parse_proxy_function_calls_text(&combined)
+}
+
+fn parse_proxy_function_calls_text(text: &str) -> Option<Vec<(String, String)>> {
+    let mut remaining = text.trim_start();
+    let first_idx = remaining.find("<function")?;
+    remaining = &remaining[first_idx..];
+
+    let mut calls = Vec::new();
+    loop {
+        let (name, after_header) = parse_proxy_function_header(remaining)?;
+        let (arguments, after_args) = parse_proxy_function_arguments(after_header)?;
+        calls.push((name, arguments));
+
+        if let Some(next_idx) = after_args.find("<function") {
+            remaining = &after_args[next_idx..];
+        } else {
+            break;
+        }
+    }
+
+    (!calls.is_empty()).then_some(calls)
+}
+
+fn parse_proxy_function_header(text: &str) -> Option<(String, &str)> {
+    if let Some(after_prefix) = text.strip_prefix("<function=") {
+        let end = after_prefix
+            .find('>')
+            .or_else(|| after_prefix.find('\n'))
+            .unwrap_or(after_prefix.len());
+        let name = after_prefix[..end].trim();
+        if name.is_empty() {
+            return None;
+        }
+        let mut remaining = &after_prefix[end..];
+        if let Some(after_gt) = remaining.strip_prefix('>') {
+            remaining = after_gt;
+        }
+        return Some((name.to_string(), remaining));
+    }
+
+    let after_prefix = text.strip_prefix("<function>")?;
+    let end = after_prefix
+        .find("</function>")
+        .or_else(|| after_prefix.find('\n'))
+        .unwrap_or(after_prefix.len());
+    let name = after_prefix[..end].trim();
+    if name.is_empty() {
+        return None;
+    }
+
+    let mut remaining = &after_prefix[end..];
+    if let Some(after_close) = remaining.strip_prefix("</function>") {
+        remaining = after_close;
+    }
+    Some((name.to_string(), remaining))
+}
+
+fn parse_proxy_function_arguments(text: &str) -> Option<(String, &str)> {
+    let mut remaining = text.trim_start();
+    if let Some(after_json) = remaining.strip_prefix("json") {
+        remaining = after_json;
+    }
+    remaining = remaining
+        .strip_prefix('\n')
+        .unwrap_or(remaining)
+        .trim_start();
+
+    if remaining.starts_with('{') {
+        let mut stream = serde_json::Deserializer::from_str(remaining).into_iter::<Value>();
+        let parsed = stream.next()?.ok()?;
+        let args = match parsed {
+            Value::Object(_) => parsed,
+            _ => return None,
+        };
+        let offset = stream.byte_offset();
+        let remaining = remaining.get(offset..)?;
+        return Some((serde_json::to_string(&args).ok()?, remaining));
+    }
+
+    if remaining.starts_with("<parameter=") {
+        return parse_proxy_parameter_arguments(remaining);
+    }
+
+    if remaining.starts_with("<parameter>") {
+        return parse_proxy_legacy_parameter_arguments(remaining);
+    }
+
+    None
+}
+
+fn parse_proxy_parameter_arguments(text: &str) -> Option<(String, &str)> {
+    let mut remaining = text;
+    let mut args = serde_json::Map::new();
+    loop {
+        let trimmed = remaining.trim_start();
+        let Some(after_prefix) = trimmed.strip_prefix("<parameter=") else {
+            break;
+        };
+        let end_name = after_prefix.find('>')?;
+        let name = after_prefix[..end_name].trim();
+        if name.is_empty() {
+            return None;
+        }
+        let after_gt = &after_prefix[end_name + 1..];
+        let (value, after_value) = after_gt.split_once("</parameter>")?;
+        args.insert(name.to_string(), Value::String(value.trim().to_string()));
+        remaining = after_value;
+    }
+
+    if args.is_empty() {
+        return None;
+    }
+    Some((serde_json::to_string(&Value::Object(args)).ok()?, remaining))
+}
+
+fn parse_proxy_legacy_parameter_arguments(text: &str) -> Option<(String, &str)> {
+    let mut remaining = text;
+    let mut args = serde_json::Map::new();
+    while let Some(start) = remaining.find("<parameter>") {
+        remaining = &remaining[start + "<parameter>".len()..];
+        let (param_name, after_name) = remaining.split_once("</parameter>")?;
+        let param_name = param_name.trim();
+        if param_name.is_empty() {
+            return None;
+        }
+
+        let value_end = after_name
+            .find("</parameter>")
+            .or_else(|| after_name.find('\n'))
+            .or_else(|| after_name.find("</function>"))
+            .or_else(|| after_name.find("<function"))
+            .unwrap_or(after_name.len());
+        let value = after_name[..value_end].trim();
+        args.insert(param_name.to_string(), Value::String(value.to_string()));
+
+        remaining = &after_name[value_end..];
+        if let Some(after_close) = remaining.strip_prefix("</parameter>") {
+            remaining = after_close;
+        }
+    }
+
+    if args.is_empty() {
+        return None;
+    }
+
+    Some((serde_json::to_string(&Value::Object(args)).ok()?, remaining))
+}
+
 #[instrument(level = "trace", skip_all)]
 pub(crate) async fn handle_output_item_done(
     ctx: &mut HandleOutputCtx,
@@ -74,7 +258,7 @@ pub(crate) async fn handle_output_item_done(
             );
 
             output.needs_follow_up = true;
-            output.tool_future = Some(tool_future);
+            output.tool_futures.push(tool_future);
         }
         // No tool call: convert messages/reasoning into turn items and mark them as complete.
         Ok(None) => {
@@ -93,9 +277,42 @@ pub(crate) async fn handle_output_item_done(
             ctx.sess
                 .record_conversation_items(&ctx.turn_context, std::slice::from_ref(&item))
                 .await;
-            let last_agent_message = last_assistant_message_from_item(&item, plan_mode);
 
-            output.last_agent_message = last_agent_message;
+            if should_parse_proxy_tool_calls(&ctx.turn_context.provider) {
+                if let Some(calls) = parse_proxy_function_calls(&item) {
+                    for (tool_name, arguments) in calls {
+                        let call = ToolCall {
+                            tool_name,
+                            call_id: format!("proxy_call_{}", Uuid::new_v4()),
+                            payload: crate::tools::context::ToolPayload::Function { arguments },
+                        };
+
+                        let payload_preview = call.payload.log_payload().into_owned();
+                        tracing::info!(
+                            thread_id = %ctx.sess.conversation_id,
+                            "ToolCall (proxy text): {} {}",
+                            call.tool_name,
+                            payload_preview
+                        );
+
+                        let cancellation_token = ctx.cancellation_token.child_token();
+                        let tool_future: InFlightFuture<'static> = Box::pin(
+                            ctx.tool_runtime
+                                .clone()
+                                .handle_tool_call(call, cancellation_token),
+                        );
+
+                        output.tool_futures.push(tool_future);
+                    }
+
+                    output.needs_follow_up = !output.tool_futures.is_empty();
+                    output.last_agent_message = None;
+                } else {
+                    output.last_agent_message = last_assistant_message_from_item(&item, plan_mode);
+                }
+            } else {
+                output.last_agent_message = last_assistant_message_from_item(&item, plan_mode);
+            }
         }
         // Guardrail: the model issued a LocalShellCall without an id; surface the error back into history.
         Err(FunctionCallError::MissingLocalShellCallId) => {
diff --git a/codex-rs/core/src/tools/handlers/list_dir.rs b/codex-rs/core/src/tools/handlers/list_dir.rs
index 5535ce0ba..1e7dd3cc8 100644
--- a/codex-rs/core/src/tools/handlers/list_dir.rs
+++ b/codex-rs/core/src/tools/handlers/list_dir.rs
@@ -37,6 +37,7 @@ fn default_depth() -> usize {
 
 #[derive(Deserialize)]
 struct ListDirArgs {
+    /// Path to the directory to list (absolute or relative to workspace CWD).
     dir_path: String,
     #[serde(default = "default_offset")]
     offset: usize,
@@ -53,7 +54,7 @@ impl ToolHandler for ListDirHandler {
     }
 
     async fn handle(&self, invocation: ToolInvocation) -> Result<ToolOutput, FunctionCallError> {
-        let ToolInvocation { payload, .. } = invocation;
+        let ToolInvocation { payload, turn, .. } = invocation;
 
         let arguments = match payload {
             ToolPayload::Function { arguments } => arguments,
@@ -91,12 +92,7 @@ impl ToolHandler for ListDirHandler {
             ));
         }
 
-        let path = PathBuf::from(&dir_path);
-        if !path.is_absolute() {
-            return Err(FunctionCallError::RespondToModel(
-                "dir_path must be an absolute path".to_string(),
-            ));
-        }
+        let path = resolve_workspace_path(&dir_path, &turn.cwd);
 
         let entries = list_dir_slice(&path, offset, limit, depth).await?;
         let mut output = Vec::with_capacity(entries.len() + 1);
@@ -109,6 +105,15 @@ impl ToolHandler for ListDirHandler {
     }
 }
 
+fn resolve_workspace_path(tool_path: &str, cwd: &Path) -> PathBuf {
+    let path = PathBuf::from(tool_path);
+    if path.is_absolute() {
+        path
+    } else {
+        cwd.join(path)
+    }
+}
+
 async fn list_dir_slice(
     path: &Path,
     offset: usize,
@@ -274,6 +279,28 @@ mod tests {
     use pretty_assertions::assert_eq;
     use tempfile::tempdir;
 
+    #[test]
+    fn resolve_workspace_path_keeps_absolute_paths() -> anyhow::Result<()> {
+        let cwd = std::env::current_dir()?;
+        let absolute = cwd.join("codex-rs");
+
+        assert_eq!(
+            resolve_workspace_path(&absolute.to_string_lossy(), &cwd),
+            absolute
+        );
+        Ok(())
+    }
+
+    #[test]
+    fn resolve_workspace_path_joins_relative_paths() -> anyhow::Result<()> {
+        let cwd = std::env::current_dir()?;
+        let relative = "codex-rs/core/src";
+        let expected = cwd.join(relative);
+
+        assert_eq!(resolve_workspace_path(relative, &cwd), expected);
+        Ok(())
+    }
+
     #[tokio::test]
     async fn lists_directory_entries() {
         let temp = tempdir().expect("create tempdir");
diff --git a/codex-rs/core/src/tools/handlers/read_file.rs b/codex-rs/core/src/tools/handlers/read_file.rs
index 59c6ced7f..7cde71b04 100644
--- a/codex-rs/core/src/tools/handlers/read_file.rs
+++ b/codex-rs/core/src/tools/handlers/read_file.rs
@@ -1,5 +1,6 @@
 use codex_protocol::models::FunctionCallOutputBody;
 use std::collections::VecDeque;
+use std::path::Path;
 use std::path::PathBuf;
 
 use async_trait::async_trait;
@@ -25,7 +26,7 @@ const COMMENT_PREFIXES: &[&str] = &["#", "//", "--"];
 /// JSON arguments accepted by the `read_file` tool handler.
 #[derive(Deserialize)]
 struct ReadFileArgs {
-    /// Absolute path to the file that will be read.
+    /// Path to the file that will be read (absolute or relative to workspace CWD).
     file_path: String,
     /// 1-indexed line number to start reading from; defaults to 1.
     #[serde(default = "defaults::offset")]
@@ -99,7 +100,7 @@ impl ToolHandler for ReadFileHandler {
     }
 
     async fn handle(&self, invocation: ToolInvocation) -> Result<ToolOutput, FunctionCallError> {
-        let ToolInvocation { payload, .. } = invocation;
+        let ToolInvocation { payload, turn, .. } = invocation;
 
         let arguments = match payload {
             ToolPayload::Function { arguments } => arguments,
@@ -132,12 +133,7 @@ impl ToolHandler for ReadFileHandler {
             ));
         }
 
-        let path = PathBuf::from(&file_path);
-        if !path.is_absolute() {
-            return Err(FunctionCallError::RespondToModel(
-                "file_path must be an absolute path".to_string(),
-            ));
-        }
+        let path = resolve_workspace_path(&file_path, &turn.cwd);
 
         let collected = match mode {
             ReadMode::Slice => slice::read(&path, offset, limit).await?,
@@ -153,6 +149,15 @@ impl ToolHandler for ReadFileHandler {
     }
 }
 
+fn resolve_workspace_path(tool_path: &str, cwd: &Path) -> PathBuf {
+    let path = PathBuf::from(tool_path);
+    if path.is_absolute() {
+        path
+    } else {
+        cwd.join(path)
+    }
+}
+
 mod slice {
     use crate::function_tool::FunctionCallError;
     use crate::tools::handlers::read_file::format_line;
@@ -491,6 +496,28 @@ mod tests {
     use pretty_assertions::assert_eq;
     use tempfile::NamedTempFile;
 
+    #[test]
+    fn resolve_workspace_path_keeps_absolute_paths() -> anyhow::Result<()> {
+        let cwd = std::env::current_dir()?;
+        let absolute = cwd.join("Cargo.toml");
+
+        assert_eq!(
+            resolve_workspace_path(&absolute.to_string_lossy(), &cwd),
+            absolute
+        );
+        Ok(())
+    }
+
+    #[test]
+    fn resolve_workspace_path_joins_relative_paths() -> anyhow::Result<()> {
+        let cwd = std::env::current_dir()?;
+        let relative = "codex-rs/core/src/lib.rs";
+        let expected = cwd.join(relative);
+
+        assert_eq!(resolve_workspace_path(relative, &cwd), expected);
+        Ok(())
+    }
+
     #[tokio::test]
     async fn reads_requested_range() -> anyhow::Result<()> {
         let mut temp = NamedTempFile::new()?;
diff --git a/codex-rs/core/src/tools/spec.rs b/codex-rs/core/src/tools/spec.rs
index 82e34298c..f73e66317 100644
--- a/codex-rs/core/src/tools/spec.rs
+++ b/codex-rs/core/src/tools/spec.rs
@@ -962,7 +962,9 @@ fn create_read_file_tool() -> ToolSpec {
         (
             "file_path".to_string(),
             JsonSchema::String {
-                description: Some("Absolute path to the file".to_string()),
+                description: Some(
+                    "Path to the file (absolute or relative to workspace CWD)".to_string(),
+                ),
             },
         ),
         (
@@ -1018,7 +1020,10 @@ fn create_list_dir_tool() -> ToolSpec {
         (
             "dir_path".to_string(),
             JsonSchema::String {
-                description: Some("Absolute path to the directory to list.".to_string()),
+                description: Some(
+                    "Path to the directory to list (absolute or relative to workspace CWD)."
+                        .to_string(),
+                ),
             },
         ),
         (
diff --git a/patches/chutes-non-openai-responses-proxy-e6e4c5fa3.patch b/patches/chutes-non-openai-responses-proxy-e6e4c5fa3.patch
new file mode 100644
index 000000000..47d14cc88
--- /dev/null
+++ b/patches/chutes-non-openai-responses-proxy-e6e4c5fa3.patch
@@ -0,0 +1,288 @@
+diff --git a/codex-rs/core/src/models_manager/model_info.rs b/codex-rs/core/src/models_manager/model_info.rs
+index 6a29cd96f..672aacd9e 100644
+--- a/codex-rs/core/src/models_manager/model_info.rs
++++ b/codex-rs/core/src/models_manager/model_info.rs
+@@ -1,3 +1,4 @@
++use codex_protocol::openai_models::ApplyPatchToolType;
+ use codex_protocol::openai_models::ConfigShellToolType;
+ use codex_protocol::openai_models::ModelInfo;
+ use codex_protocol::openai_models::ModelInstructionsVariables;
+@@ -18,6 +19,7 @@ const LOCAL_FRIENDLY_TEMPLATE: &str =
+     "You optimize for team morale and being a supportive teammate as much as code quality.";
+ const LOCAL_PRAGMATIC_TEMPLATE: &str = "You are a deeply pragmatic, effective software engineer.";
+ const PERSONALITY_PLACEHOLDER: &str = "{{ personality }}";
++const FALLBACK_EXPERIMENTAL_TOOLS: [&str; 3] = ["grep_files", "list_dir", "read_file"];
+ 
+ pub(crate) fn with_config_overrides(mut model: ModelInfo, config: &Config) -> ModelInfo {
+     if let Some(supports_reasoning_summaries) = config.model_supports_reasoning_summaries {
+@@ -72,18 +74,26 @@ pub(crate) fn model_info_from_slug(slug: &str) -> ModelInfo {
+         supports_reasoning_summaries: false,
+         support_verbosity: false,
+         default_verbosity: None,
+-        apply_patch_tool_type: None,
++        // Unknown models are frequently third-party and still need core coding tools.
++        apply_patch_tool_type: Some(ApplyPatchToolType::Function),
+         truncation_policy: TruncationPolicyConfig::bytes(10_000),
+-        supports_parallel_tool_calls: false,
++        supports_parallel_tool_calls: true,
+         context_window: Some(272_000),
+         auto_compact_token_limit: None,
+         effective_context_window_percent: 95,
+-        experimental_supported_tools: Vec::new(),
++        experimental_supported_tools: fallback_experimental_supported_tools(),
+         input_modalities: default_input_modalities(),
+         prefer_websockets: false,
+     }
+ }
+ 
++fn fallback_experimental_supported_tools() -> Vec<String> {
++    FALLBACK_EXPERIMENTAL_TOOLS
++        .into_iter()
++        .map(str::to_string)
++        .collect()
++}
++
+ fn local_personality_messages_for_slug(slug: &str) -> Option<ModelMessages> {
+     match slug {
+         "gpt-5.2-codex" | "exp-codex-personality" => Some(ModelMessages {
+@@ -99,3 +109,45 @@ fn local_personality_messages_for_slug(slug: &str) -> Option<ModelMessages> {
+         _ => None,
+     }
+ }
++
++#[cfg(test)]
++mod tests {
++    use super::*;
++    use pretty_assertions::assert_eq;
++
++    #[test]
++    fn unknown_model_fallback_enables_proxy_compatible_tools() {
++        let slug = "Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8";
++        let model = model_info_from_slug(slug);
++
++        assert_eq!(
++            model,
++            ModelInfo {
++                slug: slug.to_string(),
++                display_name: slug.to_string(),
++                description: None,
++                default_reasoning_level: None,
++                supported_reasoning_levels: Vec::new(),
++                shell_type: ConfigShellToolType::Default,
++                visibility: ModelVisibility::None,
++                supported_in_api: true,
++                priority: 99,
++                upgrade: None,
++                base_instructions: BASE_INSTRUCTIONS.to_string(),
++                model_messages: None,
++                supports_reasoning_summaries: false,
++                support_verbosity: false,
++                default_verbosity: None,
++                apply_patch_tool_type: Some(ApplyPatchToolType::Function),
++                truncation_policy: TruncationPolicyConfig::bytes(10_000),
++                supports_parallel_tool_calls: true,
++                context_window: Some(272_000),
++                auto_compact_token_limit: None,
++                effective_context_window_percent: 95,
++                experimental_supported_tools: fallback_experimental_supported_tools(),
++                input_modalities: default_input_modalities(),
++                prefer_websockets: false,
++            }
++        );
++    }
++}
+diff --git a/codex-rs/core/src/tools/handlers/list_dir.rs b/codex-rs/core/src/tools/handlers/list_dir.rs
+index 5535ce0ba..1e7dd3cc8 100644
+--- a/codex-rs/core/src/tools/handlers/list_dir.rs
++++ b/codex-rs/core/src/tools/handlers/list_dir.rs
+@@ -37,6 +37,7 @@ fn default_depth() -> usize {
+ 
+ #[derive(Deserialize)]
+ struct ListDirArgs {
++    /// Path to the directory to list (absolute or relative to workspace CWD).
+     dir_path: String,
+     #[serde(default = "default_offset")]
+     offset: usize,
+@@ -53,7 +54,7 @@ impl ToolHandler for ListDirHandler {
+     }
+ 
+     async fn handle(&self, invocation: ToolInvocation) -> Result<ToolOutput, FunctionCallError> {
+-        let ToolInvocation { payload, .. } = invocation;
++        let ToolInvocation { payload, turn, .. } = invocation;
+ 
+         let arguments = match payload {
+             ToolPayload::Function { arguments } => arguments,
+@@ -91,12 +92,7 @@ impl ToolHandler for ListDirHandler {
+             ));
+         }
+ 
+-        let path = PathBuf::from(&dir_path);
+-        if !path.is_absolute() {
+-            return Err(FunctionCallError::RespondToModel(
+-                "dir_path must be an absolute path".to_string(),
+-            ));
+-        }
++        let path = resolve_workspace_path(&dir_path, &turn.cwd);
+ 
+         let entries = list_dir_slice(&path, offset, limit, depth).await?;
+         let mut output = Vec::with_capacity(entries.len() + 1);
+@@ -109,6 +105,15 @@ impl ToolHandler for ListDirHandler {
+     }
+ }
+ 
++fn resolve_workspace_path(tool_path: &str, cwd: &Path) -> PathBuf {
++    let path = PathBuf::from(tool_path);
++    if path.is_absolute() {
++        path
++    } else {
++        cwd.join(path)
++    }
++}
++
+ async fn list_dir_slice(
+     path: &Path,
+     offset: usize,
+@@ -274,6 +279,28 @@ mod tests {
+     use pretty_assertions::assert_eq;
+     use tempfile::tempdir;
+ 
++    #[test]
++    fn resolve_workspace_path_keeps_absolute_paths() -> anyhow::Result<()> {
++        let cwd = std::env::current_dir()?;
++        let absolute = cwd.join("codex-rs");
++
++        assert_eq!(
++            resolve_workspace_path(&absolute.to_string_lossy(), &cwd),
++            absolute
++        );
++        Ok(())
++    }
++
++    #[test]
++    fn resolve_workspace_path_joins_relative_paths() -> anyhow::Result<()> {
++        let cwd = std::env::current_dir()?;
++        let relative = "codex-rs/core/src";
++        let expected = cwd.join(relative);
++
++        assert_eq!(resolve_workspace_path(relative, &cwd), expected);
++        Ok(())
++    }
++
+     #[tokio::test]
+     async fn lists_directory_entries() {
+         let temp = tempdir().expect("create tempdir");
+diff --git a/codex-rs/core/src/tools/handlers/read_file.rs b/codex-rs/core/src/tools/handlers/read_file.rs
+index 59c6ced7f..7cde71b04 100644
+--- a/codex-rs/core/src/tools/handlers/read_file.rs
++++ b/codex-rs/core/src/tools/handlers/read_file.rs
+@@ -1,5 +1,6 @@
+ use codex_protocol::models::FunctionCallOutputBody;
+ use std::collections::VecDeque;
++use std::path::Path;
+ use std::path::PathBuf;
+ 
+ use async_trait::async_trait;
+@@ -25,7 +26,7 @@ const COMMENT_PREFIXES: &[&str] = &["#", "//", "--"];
+ /// JSON arguments accepted by the `read_file` tool handler.
+ #[derive(Deserialize)]
+ struct ReadFileArgs {
+-    /// Absolute path to the file that will be read.
++    /// Path to the file that will be read (absolute or relative to workspace CWD).
+     file_path: String,
+     /// 1-indexed line number to start reading from; defaults to 1.
+     #[serde(default = "defaults::offset")]
+@@ -99,7 +100,7 @@ impl ToolHandler for ReadFileHandler {
+     }
+ 
+     async fn handle(&self, invocation: ToolInvocation) -> Result<ToolOutput, FunctionCallError> {
+-        let ToolInvocation { payload, .. } = invocation;
++        let ToolInvocation { payload, turn, .. } = invocation;
+ 
+         let arguments = match payload {
+             ToolPayload::Function { arguments } => arguments,
+@@ -132,12 +133,7 @@ impl ToolHandler for ReadFileHandler {
+             ));
+         }
+ 
+-        let path = PathBuf::from(&file_path);
+-        if !path.is_absolute() {
+-            return Err(FunctionCallError::RespondToModel(
+-                "file_path must be an absolute path".to_string(),
+-            ));
+-        }
++        let path = resolve_workspace_path(&file_path, &turn.cwd);
+ 
+         let collected = match mode {
+             ReadMode::Slice => slice::read(&path, offset, limit).await?,
+@@ -153,6 +149,15 @@ impl ToolHandler for ReadFileHandler {
+     }
+ }
+ 
++fn resolve_workspace_path(tool_path: &str, cwd: &Path) -> PathBuf {
++    let path = PathBuf::from(tool_path);
++    if path.is_absolute() {
++        path
++    } else {
++        cwd.join(path)
++    }
++}
++
+ mod slice {
+     use crate::function_tool::FunctionCallError;
+     use crate::tools::handlers::read_file::format_line;
+@@ -491,6 +496,28 @@ mod tests {
+     use pretty_assertions::assert_eq;
+     use tempfile::NamedTempFile;
+ 
++    #[test]
++    fn resolve_workspace_path_keeps_absolute_paths() -> anyhow::Result<()> {
++        let cwd = std::env::current_dir()?;
++        let absolute = cwd.join("Cargo.toml");
++
++        assert_eq!(
++            resolve_workspace_path(&absolute.to_string_lossy(), &cwd),
++            absolute
++        );
++        Ok(())
++    }
++
++    #[test]
++    fn resolve_workspace_path_joins_relative_paths() -> anyhow::Result<()> {
++        let cwd = std::env::current_dir()?;
++        let relative = "codex-rs/core/src/lib.rs";
++        let expected = cwd.join(relative);
++
++        assert_eq!(resolve_workspace_path(relative, &cwd), expected);
++        Ok(())
++    }
++
+     #[tokio::test]
+     async fn reads_requested_range() -> anyhow::Result<()> {
+         let mut temp = NamedTempFile::new()?;
+diff --git a/codex-rs/core/src/tools/spec.rs b/codex-rs/core/src/tools/spec.rs
+index 82e34298c..f73e66317 100644
+--- a/codex-rs/core/src/tools/spec.rs
++++ b/codex-rs/core/src/tools/spec.rs
+@@ -962,7 +962,9 @@ fn create_read_file_tool() -> ToolSpec {
+         (
+             "file_path".to_string(),
+             JsonSchema::String {
+-                description: Some("Absolute path to the file".to_string()),
++                description: Some(
++                    "Path to the file (absolute or relative to workspace CWD)".to_string(),
++                ),
+             },
+         ),
+         (
+@@ -1018,7 +1020,10 @@ fn create_list_dir_tool() -> ToolSpec {
+         (
+             "dir_path".to_string(),
+             JsonSchema::String {
+-                description: Some("Absolute path to the directory to list.".to_string()),
++                description: Some(
++                    "Path to the directory to list (absolute or relative to workspace CWD)."
++                        .to_string(),
++                ),
+             },
+         ),
+         (
